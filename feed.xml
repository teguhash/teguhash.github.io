<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://teguha.sh/feed.xml" rel="self" type="application/atom+xml" /><link href="https://teguha.sh/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2021-10-09T23:03:34+07:00</updated><id>https://teguha.sh/feed.xml</id><title type="html">teguha.sh</title><subtitle>A description for your page.
</subtitle><author><name>Teguh A.S. Hidayat</name></author><entry><title type="html">Why Gradient Boosting Almost Always Outperforms Random Forest</title><link href="https://teguha.sh/til/2020/11/11/why-gb-outperforms-rf/" rel="alternate" type="text/html" title="Why Gradient Boosting Almost Always Outperforms Random Forest" /><published>2020-11-11T00:00:00+07:00</published><updated>2020-11-11T00:00:00+07:00</updated><id>https://teguha.sh/til/2020/11/11/why-gb-outperforms-rf</id><content type="html" xml:base="https://teguha.sh/til/2020/11/11/why-gb-outperforms-rf/">&lt;p&gt;&lt;em&gt;TLDR: GB can reduce error via both bias and variance, RF can only reduce error by reducing variance&lt;/em&gt; &lt;br /&gt;
&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mtext&gt;error&lt;/mtext&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mtext&gt;bias&lt;/mtext&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mtext&gt;variance&lt;/mtext&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\text{error} = \text{bias} + \text{variance}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.43056em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord text&quot;&gt;&lt;span class=&quot;mord&quot;&gt;error&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.77777em;vertical-align:-0.08333em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord text&quot;&gt;&lt;span class=&quot;mord&quot;&gt;bias&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mbin&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:0.66786em;vertical-align:0em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord text&quot;&gt;&lt;span class=&quot;mord&quot;&gt;variance&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;br /&gt;
When building a Random Forest, we build a lot of &lt;em&gt;fully grown&lt;/em&gt; trees. Bias of the RF model equals to an individual tree hence the needs to build a fully grown tree with really low bias.
To reduce the overall error, we also reduce its variance by building a lot of trees.
With Gradient Boost, we build a lot of weak learners (small trees) and aggregate their results to reduce variance. And since we build a lot of these weak learners that improve upon previously built tree, we are also reducing its total bias.&lt;/p&gt;</content><author><name>Teguh A.S. Hidayat</name></author><category term="til" /><category term="machine learning" /><category term="algorithm" /><summary type="html">TLDR: GB can reduce error via both bias and variance, RF can only reduce error by reducing variance error=bias+variance\text{error} = \text{bias} + \text{variance}error=bias+variance When building a Random Forest, we build a lot of fully grown trees. Bias of the RF model equals to an individual tree hence the needs to build a fully grown tree with really low bias. To reduce the overall error, we also reduce its variance by building a lot of trees. With Gradient Boost, we build a lot of weak learners (small trees) and aggregate their results to reduce variance. And since we build a lot of these weak learners that improve upon previously built tree, we are also reducing its total bias.</summary></entry></feed>